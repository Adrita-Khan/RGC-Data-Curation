{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXt1xqa0ofBn",
        "outputId": "91a56853-0a85-419c-96af-99ee43a1e7d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q astroquery"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================#\n",
        "# RADIO GALAXY SIMBAD QUERY SCRIPT                                  #\n",
        "# ============================================================================#\n",
        "# Features:\n",
        "# - Concurrent SIMBAD queries with progress tracking\n",
        "# - Configuration-driven parameters\n",
        "# - Robust error handling with retry logic\n",
        "# - Checkpoint system for crash recovery\n",
        "# - Proper logging framework\n",
        "# - Type hints and comprehensive documentation\n",
        "# ============================================================================#\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from astroquery.simbad import Simbad\n",
        "from astropy.coordinates import SkyCoord\n",
        "from astropy import units as u\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# ============================================================================#\n",
        "# CONFIGURATION                                                              #\n",
        "# ============================================================================#\n",
        "\n",
        "CONFIG = {\n",
        "    # File paths\n",
        "    'drive_url': \"https://drive.google.com/file/d/1owgNHA6x1LMgB7uCvueUQYoYxseXJfzD/view?usp=sharing\",\n",
        "    'local_filename': \"rgz_data.csv\",\n",
        "    'output_filename': \"simbad_query_results.csv\",\n",
        "    'checkpoint_dir': \"checkpoints\",\n",
        "    'log_filename': \"simbad_query.log\",\n",
        "\n",
        "    # Query parameters\n",
        "    'search_radius_deg': 0.1,\n",
        "    'max_objects': 100,\n",
        "    'batch_size': 10,\n",
        "    'checkpoint_interval': 20,\n",
        "\n",
        "    # Network parameters\n",
        "    'timeout_seconds': 500,\n",
        "    'rate_limit_delay': 1.0,\n",
        "    'max_retries': 3,\n",
        "    'retry_backoff': 2.0,  # exponential backoff multiplier\n",
        "    'max_workers': 5,  # concurrent workers\n",
        "\n",
        "    # Validation\n",
        "    'min_ra': 0,\n",
        "    'max_ra': 360,\n",
        "    'min_dec': -90,\n",
        "    'max_dec': 90,\n",
        "}\n",
        "\n",
        "# Column name mapping for standardization\n",
        "COLUMN_MAPPING = {\n",
        "    'rgzid': 'RGZ_ID',\n",
        "    'rgz_id': 'RGZ_ID',\n",
        "    'id': 'RGZ_ID',\n",
        "    'ra': 'RA',\n",
        "    'ra_deg': 'RA',\n",
        "    'dec': 'Dec',\n",
        "    'dec_deg': 'Dec',\n",
        "    'declination': 'Dec',\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================================#\n",
        "# LOGGING SETUP                                                              #\n",
        "# ============================================================================#\n",
        "\n",
        "def setup_logging(log_file: str) -> logging.Logger:\n",
        "    \"\"\"\n",
        "    Configure logging to both console and file.\n",
        "\n",
        "    Args:\n",
        "        log_file: Path to log file\n",
        "\n",
        "    Returns:\n",
        "        Configured logger instance\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger('SIMBAD_Query')\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Console handler\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "\n",
        "    # File handler\n",
        "    file_handler = logging.FileHandler(log_file)\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Formatter\n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "    console_handler.setFormatter(formatter)\n",
        "    file_handler.setFormatter(formatter)\n",
        "\n",
        "    logger.addHandler(console_handler)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "# ============================================================================#\n",
        "# DATA LOADING AND CLEANING                                                  #\n",
        "# ============================================================================#\n",
        "\n",
        "def extract_file_id(drive_url: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract Google Drive file ID from sharing URL.\n",
        "\n",
        "    Args:\n",
        "        drive_url: Google Drive sharing URL\n",
        "\n",
        "    Returns:\n",
        "        File ID string\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If file ID cannot be extracted\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file_id = drive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
        "        if not file_id:\n",
        "            raise ValueError(\"Empty file ID extracted\")\n",
        "        return file_id\n",
        "    except (IndexError, ValueError) as e:\n",
        "        raise ValueError(f\"Cannot extract file ID from URL: {e}\")\n",
        "\n",
        "\n",
        "def download_data(drive_url: str, local_filename: str, logger: logging.Logger) -> bool:\n",
        "    \"\"\"\n",
        "    Download CSV file from Google Drive with validation.\n",
        "\n",
        "    Args:\n",
        "        drive_url: Google Drive sharing URL\n",
        "        local_filename: Local path to save file\n",
        "        logger: Logger instance\n",
        "\n",
        "    Returns:\n",
        "        True if download successful, False otherwise\n",
        "    \"\"\"\n",
        "    if os.path.exists(local_filename):\n",
        "        logger.info(f\"✓ Using cached file: {local_filename}\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Downloading CSV from Google Drive...\")\n",
        "        file_id = extract_file_id(drive_url)\n",
        "        download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "        response = requests.get(download_url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        with open(local_filename, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        logger.info(f\"✓ Downloaded file saved as {local_filename}\")\n",
        "        logger.debug(f\"File size: {os.path.getsize(local_filename)} bytes\")\n",
        "        return True\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        logger.error(f\"Failed to download file: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def standardize_columns(df: pd.DataFrame, logger: logging.Logger) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Standardize column names to expected format.\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        logger: Logger instance\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with standardized column names\n",
        "    \"\"\"\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    rename_dict = {}\n",
        "    for old_col in df.columns:\n",
        "        if old_col in COLUMN_MAPPING:\n",
        "            rename_dict[old_col] = COLUMN_MAPPING[old_col]\n",
        "\n",
        "    if rename_dict:\n",
        "        df.rename(columns=rename_dict, inplace=True)\n",
        "        logger.debug(f\"Renamed columns: {rename_dict}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_and_clean_data(\n",
        "    local_filename: str,\n",
        "    config: Dict,\n",
        "    logger: logging.Logger\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load and clean astronomical data from CSV.\n",
        "\n",
        "    Args:\n",
        "        local_filename: Path to CSV file\n",
        "        config: Configuration dictionary\n",
        "        logger: Logger instance\n",
        "\n",
        "    Returns:\n",
        "        Cleaned DataFrame\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If file doesn't exist\n",
        "        ValueError: If required columns are missing or data is invalid\n",
        "    \"\"\"\n",
        "    if not os.path.exists(local_filename):\n",
        "        raise FileNotFoundError(f\"File not found: {local_filename}\")\n",
        "\n",
        "    logger.info(f\"Loading data from {local_filename}...\")\n",
        "    df = pd.read_csv(local_filename)\n",
        "    logger.info(f\"✓ Loaded {len(df):,} rows\")\n",
        "    logger.debug(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Standardize column names\n",
        "    df = standardize_columns(df, logger)\n",
        "\n",
        "    # Verify required columns\n",
        "    required_cols = ['RGZ_ID', 'RA', 'Dec']\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "    logger.debug(f\"Before cleaning: {len(df)} rows\")\n",
        "\n",
        "    # Clean data\n",
        "    df = df.dropna(subset=required_cols)\n",
        "    logger.debug(f\"After removing NaN: {len(df)} rows\")\n",
        "\n",
        "    # Validate coordinate ranges\n",
        "    initial_len = len(df)\n",
        "    df = df[\n",
        "        (df['RA'] >= config['min_ra']) &\n",
        "        (df['RA'] <= config['max_ra']) &\n",
        "        (df['Dec'] >= config['min_dec']) &\n",
        "        (df['Dec'] <= config['max_dec'])\n",
        "    ].reset_index(drop=True)\n",
        "\n",
        "    removed = initial_len - len(df)\n",
        "    if removed > 0:\n",
        "        logger.warning(f\"Removed {removed} rows with invalid coordinates\")\n",
        "\n",
        "    # Check for duplicates\n",
        "    duplicates = df['RGZ_ID'].duplicated().sum()\n",
        "    if duplicates > 0:\n",
        "        logger.warning(f\"Found {duplicates} duplicate RGZ_IDs\")\n",
        "\n",
        "    logger.info(f\"✓ {len(df):,} valid rows after cleaning\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================================#\n",
        "# CHECKPOINT SYSTEM                                                          #\n",
        "# ============================================================================#\n",
        "\n",
        "def load_checkpoint(\n",
        "    checkpoint_dir: str,\n",
        "    logger: logging.Logger\n",
        ") -> Tuple[List[Dict], int]:\n",
        "    \"\"\"\n",
        "    Load previous progress from checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_dir: Directory containing checkpoint files\n",
        "        logger: Logger instance\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (results list, processed count)\n",
        "    \"\"\"\n",
        "    checkpoint_file = os.path.join(checkpoint_dir, \"checkpoint.json\")\n",
        "    results_file = os.path.join(checkpoint_dir, \"partial_results.csv\")\n",
        "\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        try:\n",
        "            with open(checkpoint_file, 'r') as f:\n",
        "                checkpoint = json.load(f)\n",
        "\n",
        "            processed_count = checkpoint.get('processed_count', 0)\n",
        "            logger.info(f\"✓ Loaded checkpoint: {processed_count} objects already processed\")\n",
        "\n",
        "            if os.path.exists(results_file):\n",
        "                results_df = pd.read_csv(results_file)\n",
        "                results = results_df.to_dict('records')\n",
        "                logger.info(f\"✓ Loaded {len(results)} partial results\")\n",
        "                return results, processed_count\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to load checkpoint: {e}\")\n",
        "\n",
        "    return [], 0\n",
        "\n",
        "\n",
        "def save_checkpoint(\n",
        "    checkpoint_dir: str,\n",
        "    processed_count: int,\n",
        "    results: List[Dict],\n",
        "    logger: logging.Logger\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Save progress to checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_dir: Directory to store checkpoint\n",
        "        processed_count: Number of objects processed\n",
        "        results: List of results collected so far\n",
        "        logger: Logger instance\n",
        "    \"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint_file = os.path.join(checkpoint_dir, \"checkpoint.json\")\n",
        "    results_file = os.path.join(checkpoint_dir, \"partial_results.csv\")\n",
        "\n",
        "    try:\n",
        "        # Save checkpoint metadata\n",
        "        checkpoint = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'processed_count': processed_count\n",
        "        }\n",
        "        with open(checkpoint_file, 'w') as f:\n",
        "            json.dump(checkpoint, f, indent=2)\n",
        "\n",
        "        # Save partial results\n",
        "        if results:\n",
        "            results_df = pd.DataFrame(results)\n",
        "            results_df.to_csv(results_file, index=False)\n",
        "\n",
        "        logger.debug(f\"✓ Checkpoint saved: {processed_count} processed\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save checkpoint: {e}\")\n",
        "\n",
        "\n",
        "# ============================================================================#\n",
        "# SIMBAD QUERYING                                                            #\n",
        "# ============================================================================#\n",
        "\n",
        "def query_simbad_single(\n",
        "    row: pd.Series,\n",
        "    config: Dict,\n",
        "    logger: logging.Logger,\n",
        "    retry_count: int = 0\n",
        ") -> Optional[List[Dict]]:\n",
        "    \"\"\"\n",
        "    Query SIMBAD for a single object with retry logic.\n",
        "\n",
        "    Args:\n",
        "        row: DataFrame row containing RGZ_ID, RA, Dec\n",
        "        config: Configuration dictionary\n",
        "        logger: Logger instance\n",
        "        retry_count: Current retry attempt\n",
        "\n",
        "    Returns:\n",
        "        List of result dictionaries or None if failed\n",
        "    \"\"\"\n",
        "    rgz_id = row['RGZ_ID']\n",
        "    ra = row['RA']\n",
        "    dec = row['Dec']\n",
        "\n",
        "    try:\n",
        "        coord = SkyCoord(ra, dec, unit=(u.deg, u.deg), frame='icrs')\n",
        "        simbad = Simbad()\n",
        "        simbad.TIMEOUT = config['timeout_seconds']\n",
        "\n",
        "        result_table = simbad.query_region(\n",
        "            coord,\n",
        "            radius=config['search_radius_deg'] * u.deg\n",
        "        )\n",
        "\n",
        "        if result_table is None:\n",
        "            logger.debug(f\"No SIMBAD matches for RGZ_ID {rgz_id}\")\n",
        "            return []\n",
        "\n",
        "        results = []\n",
        "        for record in result_table:\n",
        "            result_dict = {\n",
        "                'RGZ_ID': rgz_id,\n",
        "                'RA_input': ra,\n",
        "                'Dec_input': dec,\n",
        "                'Main_ID': record['main_id'],\n",
        "                'RA_simbad': record['ra'],\n",
        "                'Dec_simbad': record['dec'],\n",
        "                'Error_Maj': record['coo_err_maj'],\n",
        "                'Error_Min': record['coo_err_min'],\n",
        "                'Error_Angle': record['coo_err_angle'],\n",
        "                'Wavelength': record['coo_wavelength'],\n",
        "                'Bibcode': record['coo_bibcode'],\n",
        "                'Query_Timestamp': datetime.now().isoformat(),\n",
        "                'Match_Status': 'Success'\n",
        "            }\n",
        "            results.append(result_dict)\n",
        "\n",
        "        logger.debug(f\"RGZ_ID {rgz_id}: Found {len(results)} SIMBAD match(es)\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.debug(f\"Query error for RGZ_ID {rgz_id}: {type(e).__name__}: {e}\")\n",
        "\n",
        "        # Retry with exponential backoff\n",
        "        if retry_count < config['max_retries']:\n",
        "            wait_time = config['rate_limit_delay'] * (config['retry_backoff'] ** retry_count)\n",
        "            logger.debug(f\"Retrying RGZ_ID {rgz_id} in {wait_time:.1f}s (attempt {retry_count + 1})\")\n",
        "            time.sleep(wait_time)\n",
        "            return query_simbad_single(row, config, logger, retry_count + 1)\n",
        "        else:\n",
        "            logger.warning(f\"Failed to query RGZ_ID {rgz_id} after {config['max_retries']} retries\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def query_simbad_batch(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict,\n",
        "    logger: logging.Logger,\n",
        "    start_idx: int = 0\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Query SIMBAD for multiple objects using concurrent requests.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with coordinates\n",
        "        config: Configuration dictionary\n",
        "        logger: Logger instance\n",
        "        start_idx: Starting index for processing\n",
        "\n",
        "    Returns:\n",
        "        List of all results\n",
        "    \"\"\"\n",
        "    max_objects = min(config['max_objects'], len(df))\n",
        "    df_subset = df.iloc[start_idx:start_idx + max_objects].reset_index(drop=True)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    logger.info(f\"Querying SIMBAD for {len(df_subset)} objects (workers: {config['max_workers']})\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=config['max_workers']) as executor:\n",
        "        # Submit all tasks\n",
        "        future_to_idx = {\n",
        "            executor.submit(query_simbad_single, row, config, logger): idx\n",
        "            for idx, (_, row) in enumerate(df_subset.iterrows())\n",
        "        }\n",
        "\n",
        "        # Process completed tasks with progress bar\n",
        "        with tqdm(total=len(df_subset), desc=\"SIMBAD Queries\") as pbar:\n",
        "            for future in as_completed(future_to_idx):\n",
        "                idx = future_to_idx[future]\n",
        "                try:\n",
        "                    results = future.result()\n",
        "                    if results is not None:\n",
        "                        all_results.extend(results)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Worker error at index {idx}: {e}\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "                # Save checkpoint periodically\n",
        "                if (idx + 1) % config['checkpoint_interval'] == 0:\n",
        "                    save_checkpoint(\n",
        "                        config['checkpoint_dir'],\n",
        "                        start_idx + idx + 1,\n",
        "                        all_results,\n",
        "                        logger\n",
        "                    )\n",
        "\n",
        "    logger.info(f\"✓ Completed {len(df_subset)} queries, found {len(all_results)} SIMBAD matches\")\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# ============================================================================#\n",
        "# RESULTS PROCESSING AND OUTPUT                                              #\n",
        "# ============================================================================#\n",
        "\n",
        "def generate_summary_statistics(\n",
        "    df_input: pd.DataFrame,\n",
        "    results: List[Dict],\n",
        "    logger: logging.Logger\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generate summary statistics for query results.\n",
        "\n",
        "    Args:\n",
        "        df_input: Original input DataFrame\n",
        "        results: List of query results\n",
        "        logger: Logger instance\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of statistics\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        stats = {\n",
        "            'total_objects_queried': len(df_input),\n",
        "            'total_matches_found': 0,\n",
        "            'match_rate': 0.0,\n",
        "            'avg_matches_per_object': 0.0,\n",
        "            'objects_with_matches': 0,\n",
        "            'objects_without_matches': len(df_input)\n",
        "        }\n",
        "    else:\n",
        "        results_df = pd.DataFrame(results)\n",
        "        matched_objects = results_df['RGZ_ID'].nunique()\n",
        "\n",
        "        stats = {\n",
        "            'total_objects_queried': len(df_input),\n",
        "            'total_matches_found': len(results),\n",
        "            'match_rate': 100.0 * matched_objects / len(df_input),\n",
        "            'avg_matches_per_object': len(results) / matched_objects if matched_objects > 0 else 0,\n",
        "            'objects_with_matches': matched_objects,\n",
        "            'objects_without_matches': len(df_input) - matched_objects\n",
        "        }\n",
        "\n",
        "    logger.info(\"\\n\" + \"=\"*60)\n",
        "    logger.info(\"QUERY SUMMARY STATISTICS\")\n",
        "    logger.info(\"=\"*60)\n",
        "    for key, value in stats.items():\n",
        "        if 'rate' in key or 'avg' in key:\n",
        "            logger.info(f\"{key}: {value:.2f}%\") if 'rate' in key else logger.info(f\"{key}: {value:.2f}\")\n",
        "        else:\n",
        "            logger.info(f\"{key}: {value}\")\n",
        "    logger.info(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def save_results(\n",
        "    results: List[Dict],\n",
        "    output_filename: str,\n",
        "    logger: logging.Logger\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Save query results to CSV file.\n",
        "\n",
        "    Args:\n",
        "        results: List of result dictionaries\n",
        "        output_filename: Output file path\n",
        "        logger: Logger instance\n",
        "\n",
        "    Returns:\n",
        "        True if save successful, False otherwise\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        logger.warning(\"No results to save\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df.to_csv(output_filename, index=False)\n",
        "        logger.info(f\"✓ Results saved to {output_filename}\")\n",
        "        logger.debug(f\"Shape: {results_df.shape}, Columns: {list(results_df.columns)}\")\n",
        "\n",
        "        # Clean up checkpoint after successful completion\n",
        "        checkpoint_file = os.path.join(CONFIG['checkpoint_dir'], \"checkpoint.json\")\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            os.remove(checkpoint_file)\n",
        "            logger.debug(\"✓ Checkpoint cleaned up after successful completion\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save results: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# ============================================================================#\n",
        "# MAIN EXECUTION                                                             #\n",
        "# ============================================================================#\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "\n",
        "    # Setup\n",
        "    print(\"=\"*80)\n",
        "    print(\"RADIO GALAXY SIMBAD QUERY - IMPROVED VERSION\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    logger = setup_logging(CONFIG['log_filename'])\n",
        "    logger.info(\"Script started\")\n",
        "    logger.debug(f\"Configuration: {json.dumps(CONFIG, indent=2)}\")\n",
        "\n",
        "    try:\n",
        "        # Download data\n",
        "        if not download_data(CONFIG['drive_url'], CONFIG['local_filename'], logger):\n",
        "            raise RuntimeError(\"Failed to download data file\")\n",
        "\n",
        "        # Load and clean data\n",
        "        df = load_and_clean_data(CONFIG['local_filename'], CONFIG, logger)\n",
        "\n",
        "        # Load checkpoint if available\n",
        "        result_data, processed_count = load_checkpoint(CONFIG['checkpoint_dir'], logger)\n",
        "\n",
        "        # Query SIMBAD\n",
        "        new_results = query_simbad_batch(df, CONFIG, logger, start_idx=processed_count)\n",
        "        result_data.extend(new_results)\n",
        "\n",
        "        # Generate statistics\n",
        "        stats = generate_summary_statistics(df, result_data, logger)\n",
        "\n",
        "        # Save results\n",
        "        if save_results(result_data, CONFIG['output_filename'], logger):\n",
        "            logger.info(\"✓ Script completed successfully\")\n",
        "        else:\n",
        "            logger.error(\"✗ Script failed to save results\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Script failed with error: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXOs4MJyohBn",
        "outputId": "9e05bfb5-4c3e-466e-e779-8c4e8b33c7c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 18:11:53,760 - SIMBAD_Query - INFO - Script started\n",
            "INFO:SIMBAD_Query:Script started\n",
            "DEBUG:SIMBAD_Query:Configuration: {\n",
            "  \"drive_url\": \"https://drive.google.com/file/d/1owgNHA6x1LMgB7uCvueUQYoYxseXJfzD/view?usp=sharing\",\n",
            "  \"local_filename\": \"rgz_data.csv\",\n",
            "  \"output_filename\": \"simbad_query_results.csv\",\n",
            "  \"checkpoint_dir\": \"checkpoints\",\n",
            "  \"log_filename\": \"simbad_query.log\",\n",
            "  \"search_radius_deg\": 0.1,\n",
            "  \"max_objects\": 100,\n",
            "  \"batch_size\": 10,\n",
            "  \"checkpoint_interval\": 20,\n",
            "  \"timeout_seconds\": 500,\n",
            "  \"rate_limit_delay\": 1.0,\n",
            "  \"max_retries\": 3,\n",
            "  \"retry_backoff\": 2.0,\n",
            "  \"max_workers\": 5,\n",
            "  \"min_ra\": 0,\n",
            "  \"max_ra\": 360,\n",
            "  \"min_dec\": -90,\n",
            "  \"max_dec\": 90\n",
            "}\n",
            "2025-11-01 18:11:53,773 - SIMBAD_Query - INFO - Downloading CSV from Google Drive...\n",
            "INFO:SIMBAD_Query:Downloading CSV from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "RADIO GALAXY SIMBAD QUERY - IMPROVED VERSION\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 18:11:55,181 - SIMBAD_Query - INFO - ✓ Downloaded file saved as rgz_data.csv\n",
            "INFO:SIMBAD_Query:✓ Downloaded file saved as rgz_data.csv\n",
            "DEBUG:SIMBAD_Query:File size: 5799087 bytes\n",
            "2025-11-01 18:11:55,193 - SIMBAD_Query - INFO - Loading data from rgz_data.csv...\n",
            "INFO:SIMBAD_Query:Loading data from rgz_data.csv...\n",
            "2025-11-01 18:11:55,549 - SIMBAD_Query - INFO - ✓ Loaded 100,185 rows\n",
            "INFO:SIMBAD_Query:✓ Loaded 100,185 rows\n",
            "DEBUG:SIMBAD_Query:Columns: ['CatID', 'RGZID', 'ZooniverseID', 'RA', 'Dec']\n",
            "DEBUG:SIMBAD_Query:Renamed columns: {'rgzid': 'RGZ_ID', 'ra': 'RA', 'dec': 'Dec'}\n",
            "DEBUG:SIMBAD_Query:Before cleaning: 100185 rows\n",
            "DEBUG:SIMBAD_Query:After removing NaN: 100185 rows\n",
            "2025-11-01 18:11:55,706 - SIMBAD_Query - WARNING - Found 456 duplicate RGZ_IDs\n",
            "WARNING:SIMBAD_Query:Found 456 duplicate RGZ_IDs\n",
            "2025-11-01 18:11:55,714 - SIMBAD_Query - INFO - ✓ 100,185 valid rows after cleaning\n",
            "INFO:SIMBAD_Query:✓ 100,185 valid rows after cleaning\n",
            "2025-11-01 18:11:55,718 - SIMBAD_Query - INFO - Querying SIMBAD for 100 objects (workers: 5)\n",
            "INFO:SIMBAD_Query:Querying SIMBAD for 100 objects (workers: 5)\n",
            "SIMBAD Queries:   0%|          | 0/100 [00:00<?, ?it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J143121.3+232251: Found 5 SIMBAD match(es)\n",
            "SIMBAD Queries:   1%|          | 1/100 [00:01<02:25,  1.47s/it]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J140600.1+232249: Found 10 SIMBAD match(es)\n",
            "DEBUG:SIMBAD_Query:RGZ_ID RGZ_J105512.0+232306: Found 9 SIMBAD match(es)\n",
            "SIMBAD Queries:   3%|▎         | 3/100 [00:01<00:53,  1.83it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J084831.6+232245: Found 14 SIMBAD match(es)\n",
            "SIMBAD Queries:   4%|▍         | 4/100 [00:02<00:45,  2.12it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J171515.3+232247: Found 22 SIMBAD match(es)\n",
            "SIMBAD Queries:   5%|▌         | 5/100 [00:02<00:41,  2.27it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J164643.0+232254: Found 5 SIMBAD match(es)\n",
            "SIMBAD Queries:   6%|▌         | 6/100 [00:02<00:37,  2.48it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J091550.4+232254: Found 6 SIMBAD match(es)\n",
            "SIMBAD Queries:   7%|▋         | 7/100 [00:03<00:36,  2.54it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J095705.6+232253: Found 9 SIMBAD match(es)\n",
            "SIMBAD Queries:   8%|▊         | 8/100 [00:03<00:34,  2.63it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J082451.6+232254: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:   9%|▉         | 9/100 [00:04<00:33,  2.70it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J111147.3+232253: Found 4 SIMBAD match(es)\n",
            "SIMBAD Queries:  10%|█         | 10/100 [00:04<00:32,  2.73it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J122516.6+232253: Found 15 SIMBAD match(es)\n",
            "SIMBAD Queries:  11%|█         | 11/100 [00:04<00:33,  2.68it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J122520.0+232317: Found 16 SIMBAD match(es)\n",
            "SIMBAD Queries:  12%|█▏        | 12/100 [00:05<00:34,  2.55it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J162121.2+232242: Found 4 SIMBAD match(es)\n",
            "SIMBAD Queries:  13%|█▎        | 13/100 [00:05<00:30,  2.88it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J094624.4+232241: Found 6 SIMBAD match(es)\n",
            "SIMBAD Queries:  14%|█▍        | 14/100 [00:05<00:30,  2.82it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J082547.2+232238: Found 9 SIMBAD match(es)\n",
            "SIMBAD Queries:  15%|█▌        | 15/100 [00:06<00:29,  2.90it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J084047.3+232239: Found 18 SIMBAD match(es)\n",
            "SIMBAD Queries:  16%|█▌        | 16/100 [00:06<00:28,  2.93it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J142819.8+232236: Found 7 SIMBAD match(es)\n",
            "SIMBAD Queries:  17%|█▋        | 17/100 [00:06<00:28,  2.95it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J074814.2+232234: Found 32 SIMBAD match(es)\n",
            "SIMBAD Queries:  18%|█▊        | 18/100 [00:07<00:27,  2.94it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J125702.4+232239: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  19%|█▉        | 19/100 [00:07<00:27,  2.96it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J084934.7+232234: Found 7 SIMBAD match(es)\n",
            "SIMBAD Queries:  20%|██        | 20/100 [00:07<00:27,  2.95it/s]DEBUG:SIMBAD_Query:✓ Checkpoint saved: 20 processed\n",
            "DEBUG:SIMBAD_Query:RGZ_ID RGZ_J122835.8+232234: Found 8 SIMBAD match(es)\n",
            "SIMBAD Queries:  21%|██        | 21/100 [00:08<00:26,  2.96it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J143015.2+232243: Found 15 SIMBAD match(es)\n",
            "SIMBAD Queries:  22%|██▏       | 22/100 [00:08<00:26,  2.96it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J151852.1+232235: Found 11 SIMBAD match(es)\n",
            "SIMBAD Queries:  23%|██▎       | 23/100 [00:08<00:25,  2.97it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J093204.8+232229: Found 18 SIMBAD match(es)\n",
            "SIMBAD Queries:  24%|██▍       | 24/100 [00:09<00:25,  2.97it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J144502.6+232227: Found 13 SIMBAD match(es)\n",
            "SIMBAD Queries:  25%|██▌       | 25/100 [00:09<00:25,  2.99it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J165738.2+232230: Found 20 SIMBAD match(es)\n",
            "SIMBAD Queries:  26%|██▌       | 26/100 [00:09<00:24,  2.99it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J142827.4+232231: Found 8 SIMBAD match(es)\n",
            "SIMBAD Queries:  27%|██▋       | 27/100 [00:10<00:24,  2.99it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J120519.5+232225: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  28%|██▊       | 28/100 [00:10<00:24,  2.99it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J123725.7+232223: Found 8 SIMBAD match(es)\n",
            "SIMBAD Queries:  29%|██▉       | 29/100 [00:10<00:26,  2.67it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J115054.2+232227: Found 33 SIMBAD match(es)\n",
            "SIMBAD Queries:  30%|███       | 30/100 [00:11<00:22,  3.11it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J103106.8+232230: Found 7 SIMBAD match(es)\n",
            "SIMBAD Queries:  31%|███       | 31/100 [00:11<00:22,  3.09it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J074056.2+232225: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  32%|███▏      | 32/100 [00:11<00:22,  3.05it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J123725.7+232223: Found 8 SIMBAD match(es)\n",
            "SIMBAD Queries:  33%|███▎      | 33/100 [00:12<00:22,  3.03it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J111121.5+232220: Found 15 SIMBAD match(es)\n",
            "SIMBAD Queries:  34%|███▍      | 34/100 [00:12<00:21,  3.02it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J164239.6+232217: Found 10 SIMBAD match(es)\n",
            "SIMBAD Queries:  35%|███▌      | 35/100 [00:12<00:21,  3.03it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J152256.1+232216: Found 10 SIMBAD match(es)\n",
            "SIMBAD Queries:  36%|███▌      | 36/100 [00:13<00:21,  3.02it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J142748.0+232212: Found 13 SIMBAD match(es)\n",
            "SIMBAD Queries:  37%|███▋      | 37/100 [00:13<00:20,  3.01it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J125339.7+232208: Found 8 SIMBAD match(es)\n",
            "SIMBAD Queries:  38%|███▊      | 38/100 [00:13<00:20,  3.02it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J104322.2+232209: Found 8 SIMBAD match(es)\n",
            "SIMBAD Queries:  39%|███▉      | 39/100 [00:14<00:20,  3.01it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J092913.5+232156: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  40%|████      | 40/100 [00:14<00:19,  3.02it/s]DEBUG:SIMBAD_Query:✓ Checkpoint saved: 40 processed\n",
            "DEBUG:SIMBAD_Query:RGZ_ID RGZ_J100341.7+232159: Found 8 SIMBAD match(es)\n",
            "SIMBAD Queries:  41%|████      | 41/100 [00:14<00:19,  3.02it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J171929.9+232157: Found 4 SIMBAD match(es)\n",
            "SIMBAD Queries:  42%|████▏     | 42/100 [00:15<00:19,  3.03it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J151003.1+232200: Found 19 SIMBAD match(es)\n",
            "SIMBAD Queries:  43%|████▎     | 43/100 [00:15<00:19,  2.99it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J131640.8+232200: Found 13 SIMBAD match(es)\n",
            "SIMBAD Queries:  44%|████▍     | 44/100 [00:15<00:18,  3.01it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J110408.9+232147: Found 6 SIMBAD match(es)\n",
            "SIMBAD Queries:  45%|████▌     | 45/100 [00:16<00:19,  2.77it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J073854.4+232200: Found 7 SIMBAD match(es)\n",
            "SIMBAD Queries:  46%|████▌     | 46/100 [00:16<00:18,  2.86it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J144838.3+232157: Found 11 SIMBAD match(es)\n",
            "SIMBAD Queries:  47%|████▋     | 47/100 [00:16<00:18,  2.89it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J100341.2+232159: Found 8 SIMBAD match(es)\n",
            "SIMBAD Queries:  48%|████▊     | 48/100 [00:17<00:17,  2.94it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J123226.6+232154: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  49%|████▉     | 49/100 [00:17<00:17,  2.95it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J074318.6+232139: Found 10 SIMBAD match(es)\n",
            "SIMBAD Queries:  50%|█████     | 50/100 [00:17<00:16,  2.95it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J073004.8+232151: Found 5 SIMBAD match(es)\n",
            "SIMBAD Queries:  51%|█████     | 51/100 [00:18<00:16,  2.97it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J151443.2+232144: Found 6 SIMBAD match(es)\n",
            "SIMBAD Queries:  52%|█████▏    | 52/100 [00:18<00:16,  2.98it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J081054.1+232145: Found 14 SIMBAD match(es)\n",
            "SIMBAD Queries:  53%|█████▎    | 53/100 [00:18<00:15,  2.97it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J075448.0+232138: Found 21 SIMBAD match(es)\n",
            "SIMBAD Queries:  54%|█████▍    | 54/100 [00:19<00:15,  2.96it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J102024.6+232141: Found 9 SIMBAD match(es)\n",
            "SIMBAD Queries:  55%|█████▌    | 55/100 [00:19<00:15,  2.94it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J122848.3+232136: Found 19 SIMBAD match(es)\n",
            "SIMBAD Queries:  56%|█████▌    | 56/100 [00:19<00:15,  2.92it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J132140.9+232129: Found 31 SIMBAD match(es)\n",
            "SIMBAD Queries:  57%|█████▋    | 57/100 [00:20<00:14,  2.96it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J101237.9+232113: Found 17 SIMBAD match(es)\n",
            "SIMBAD Queries:  58%|█████▊    | 58/100 [00:20<00:14,  2.97it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J143211.8+232128: Found 18 SIMBAD match(es)\n",
            "SIMBAD Queries:  59%|█████▉    | 59/100 [00:20<00:13,  2.98it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J101214.0+232117: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  60%|██████    | 60/100 [00:21<00:13,  3.00it/s]DEBUG:SIMBAD_Query:✓ Checkpoint saved: 60 processed\n",
            "DEBUG:SIMBAD_Query:RGZ_ID RGZ_J154832.9+232129: Found 14 SIMBAD match(es)\n",
            "SIMBAD Queries:  61%|██████    | 61/100 [00:21<00:13,  2.99it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J131949.7+232125: Found 15 SIMBAD match(es)\n",
            "SIMBAD Queries:  62%|██████▏   | 62/100 [00:22<00:14,  2.69it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J094009.9+232114: Found 6 SIMBAD match(es)\n",
            "SIMBAD Queries:  63%|██████▎   | 63/100 [00:22<00:13,  2.79it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J120505.8+232051: Found 14 SIMBAD match(es)\n",
            "SIMBAD Queries:  64%|██████▍   | 64/100 [00:22<00:12,  2.83it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J113949.0+232115: Found 13 SIMBAD match(es)\n",
            "SIMBAD Queries:  65%|██████▌   | 65/100 [00:23<00:12,  2.89it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J085629.7+232122: Found 21 SIMBAD match(es)\n",
            "SIMBAD Queries:  66%|██████▌   | 66/100 [00:23<00:11,  2.92it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J090215.9+232113: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  67%|██████▋   | 67/100 [00:23<00:11,  2.94it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J155647.9+232109: Found 11 SIMBAD match(es)\n",
            "SIMBAD Queries:  68%|██████▊   | 68/100 [00:24<00:10,  2.96it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J162439.0+232112: Found 7 SIMBAD match(es)\n",
            "SIMBAD Queries:  69%|██████▉   | 69/100 [00:24<00:10,  2.96it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J152105.3+232102: Found 6 SIMBAD match(es)\n",
            "SIMBAD Queries:  70%|███████   | 70/100 [00:24<00:10,  2.98it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J162753.0+232100: Found 14 SIMBAD match(es)\n",
            "SIMBAD Queries:  71%|███████   | 71/100 [00:25<00:09,  2.97it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J090625.7+232107: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  72%|███████▏  | 72/100 [00:25<00:09,  2.99it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J092253.9+232133: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  73%|███████▎  | 73/100 [00:25<00:09,  2.98it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J164449.9+232102: Found 10 SIMBAD match(es)\n",
            "SIMBAD Queries:  74%|███████▍  | 74/100 [00:26<00:08,  3.01it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J161444.2+232059: Found 30 SIMBAD match(es)\n",
            "SIMBAD Queries:  75%|███████▌  | 75/100 [00:26<00:08,  3.00it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J160436.1+232046: Found 20 SIMBAD match(es)\n",
            "SIMBAD Queries:  76%|███████▌  | 76/100 [00:26<00:07,  3.01it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J160720.6+232055: Found 15 SIMBAD match(es)\n",
            "SIMBAD Queries:  77%|███████▋  | 77/100 [00:27<00:07,  3.01it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J112434.1+232055: Found 5 SIMBAD match(es)\n",
            "SIMBAD Queries:  78%|███████▊  | 78/100 [00:27<00:07,  2.94it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J162752.9+232100: Found 14 SIMBAD match(es)\n",
            "SIMBAD Queries:  79%|███████▉  | 79/100 [00:27<00:07,  2.95it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J095429.0+232126: Found 9 SIMBAD match(es)\n",
            "SIMBAD Queries:  80%|████████  | 80/100 [00:28<00:06,  2.99it/s]DEBUG:SIMBAD_Query:✓ Checkpoint saved: 80 processed\n",
            "DEBUG:SIMBAD_Query:RGZ_ID RGZ_J135232.9+232046: Found 7 SIMBAD match(es)\n",
            "SIMBAD Queries:  81%|████████  | 81/100 [00:28<00:06,  3.00it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J090626.2+232056: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  82%|████████▏ | 82/100 [00:28<00:06,  3.00it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J153445.1+232048: Found 29 SIMBAD match(es)\n",
            "SIMBAD Queries:  83%|████████▎ | 83/100 [00:29<00:05,  2.97it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J133128.0+232040: Found 11 SIMBAD match(es)\n",
            "SIMBAD Queries:  84%|████████▍ | 84/100 [00:29<00:05,  3.00it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J162158.8+232043: Found 9 SIMBAD match(es)\n",
            "SIMBAD Queries:  85%|████████▌ | 85/100 [00:30<00:07,  2.08it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J085338.3+232105: Found 62 SIMBAD match(es)\n",
            "SIMBAD Queries:  86%|████████▌ | 86/100 [00:30<00:06,  2.28it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J164844.3+232029: Found 9 SIMBAD match(es)\n",
            "SIMBAD Queries:  87%|████████▋ | 87/100 [00:31<00:06,  2.12it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J170128.3+232036: Found 10 SIMBAD match(es)\n",
            "SIMBAD Queries:  88%|████████▊ | 88/100 [00:31<00:05,  2.31it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J093127.7+232035: Found 11 SIMBAD match(es)\n",
            "SIMBAD Queries:  89%|████████▉ | 89/100 [00:31<00:04,  2.29it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J161647.4+232035: Found 14 SIMBAD match(es)\n",
            "SIMBAD Queries:  90%|█████████ | 90/100 [00:32<00:03,  2.68it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J132032.1+232023: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries:  91%|█████████ | 91/100 [00:32<00:03,  2.78it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J120320.5+232029: Found 11 SIMBAD match(es)\n",
            "SIMBAD Queries:  92%|█████████▏| 92/100 [00:32<00:02,  2.84it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J072809.0+232026: Found 7 SIMBAD match(es)\n",
            "SIMBAD Queries:  93%|█████████▎| 93/100 [00:33<00:02,  2.89it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J080535.6+232029: Found 13 SIMBAD match(es)\n",
            "SIMBAD Queries:  94%|█████████▍| 94/100 [00:33<00:02,  2.62it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J131613.8+232025: Found 8 SIMBAD match(es)\n",
            "SIMBAD Queries:  95%|█████████▌| 95/100 [00:33<00:01,  2.73it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J120414.0+232021: Found 10 SIMBAD match(es)\n",
            "SIMBAD Queries:  96%|█████████▌| 96/100 [00:34<00:01,  2.81it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J133114.8+232016: Found 9 SIMBAD match(es)\n",
            "SIMBAD Queries:  97%|█████████▋| 97/100 [00:34<00:01,  2.88it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J164248.6+232022: Found 9 SIMBAD match(es)\n",
            "SIMBAD Queries:  98%|█████████▊| 98/100 [00:34<00:00,  2.91it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J104003.2+232009: Found 5 SIMBAD match(es)\n",
            "SIMBAD Queries:  99%|█████████▉| 99/100 [00:35<00:00,  2.94it/s]DEBUG:SIMBAD_Query:RGZ_ID RGZ_J073906.2+232006: Found 12 SIMBAD match(es)\n",
            "SIMBAD Queries: 100%|██████████| 100/100 [00:35<00:00,  2.96it/s]DEBUG:SIMBAD_Query:✓ Checkpoint saved: 100 processed\n",
            "SIMBAD Queries: 100%|██████████| 100/100 [00:35<00:00,  2.81it/s]\n",
            "2025-11-01 18:12:31,430 - SIMBAD_Query - INFO - ✓ Completed 100 queries, found 1251 SIMBAD matches\n",
            "INFO:SIMBAD_Query:✓ Completed 100 queries, found 1251 SIMBAD matches\n",
            "2025-11-01 18:12:31,442 - SIMBAD_Query - INFO - \n",
            "============================================================\n",
            "INFO:SIMBAD_Query:\n",
            "============================================================\n",
            "2025-11-01 18:12:31,444 - SIMBAD_Query - INFO - QUERY SUMMARY STATISTICS\n",
            "INFO:SIMBAD_Query:QUERY SUMMARY STATISTICS\n",
            "2025-11-01 18:12:31,446 - SIMBAD_Query - INFO - ============================================================\n",
            "INFO:SIMBAD_Query:============================================================\n",
            "2025-11-01 18:12:31,448 - SIMBAD_Query - INFO - total_objects_queried: 100185\n",
            "INFO:SIMBAD_Query:total_objects_queried: 100185\n",
            "2025-11-01 18:12:31,450 - SIMBAD_Query - INFO - total_matches_found: 1251\n",
            "INFO:SIMBAD_Query:total_matches_found: 1251\n",
            "2025-11-01 18:12:31,452 - SIMBAD_Query - INFO - match_rate: 0.10%\n",
            "INFO:SIMBAD_Query:match_rate: 0.10%\n",
            "2025-11-01 18:12:31,457 - SIMBAD_Query - INFO - avg_matches_per_object: 12.64\n",
            "INFO:SIMBAD_Query:avg_matches_per_object: 12.64\n",
            "2025-11-01 18:12:31,459 - SIMBAD_Query - INFO - objects_with_matches: 99\n",
            "INFO:SIMBAD_Query:objects_with_matches: 99\n",
            "2025-11-01 18:12:31,461 - SIMBAD_Query - INFO - objects_without_matches: 100086\n",
            "INFO:SIMBAD_Query:objects_without_matches: 100086\n",
            "2025-11-01 18:12:31,463 - SIMBAD_Query - INFO - ============================================================\n",
            "\n",
            "INFO:SIMBAD_Query:============================================================\n",
            "\n",
            "2025-11-01 18:12:31,498 - SIMBAD_Query - INFO - ✓ Results saved to simbad_query_results.csv\n",
            "INFO:SIMBAD_Query:✓ Results saved to simbad_query_results.csv\n",
            "DEBUG:SIMBAD_Query:Shape: (1251, 13), Columns: ['RGZ_ID', 'RA_input', 'Dec_input', 'Main_ID', 'RA_simbad', 'Dec_simbad', 'Error_Maj', 'Error_Min', 'Error_Angle', 'Wavelength', 'Bibcode', 'Query_Timestamp', 'Match_Status']\n",
            "DEBUG:SIMBAD_Query:✓ Checkpoint cleaned up after successful completion\n",
            "2025-11-01 18:12:31,507 - SIMBAD_Query - INFO - ✓ Script completed successfully\n",
            "INFO:SIMBAD_Query:✓ Script completed successfully\n"
          ]
        }
      ]
    }
  ]
}